{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creates a train test split using 20% of the data from each month to train and 5% of the data for testing from\n",
    "2018. This is the same ratio as an 80/20 split. We sampled from every month as we expect there to be a seasonal\n",
    "component to our model. Full datasets are to large to store on git, so they are on my local machine, and the \n",
    "splits are created, stored as a pickle file, and pushed to git\n",
    "\"\"\"\n",
    "\n",
    "train_set, test_set = pd.DataFrame(), pd.DataFrame()\n",
    "airports = ['LAX','JFK', 'ATL', 'DFW', 'DEN', 'SFO','SEA','LAS','MCO']\n",
    "files = ['april.csv', 'august.csv', 'dec.csv', 'feb.csv', 'jan.csv', 'july.csv', 'june.csv', 'march.csv',\n",
    "         'may.csv',  'nov.csv',  'oct.csv', 'sept.csv']\n",
    "c = 0\n",
    "for filename in files:\n",
    "    df = pd.read_csv(\"/home/jackson/data/\" + filename)\n",
    "    df = df[(df['ORIGIN'].isin(airports)) & (df['DEST'].isin(airports))]\n",
    "    if(train_set.empty):\n",
    "        train_set = df.sample(frac=0.20, random_state=7)\n",
    "        test_set = df.sample(frac=0.05, random_state=7)\n",
    "    else:\n",
    "        train_set = pd.concat([train_set,df.sample(frac=0.20, random_state=7)])\n",
    "        test_set = pd.concat([test_set, df.sample(frac=0.05, random_state=7)])\n",
    "\n",
    "#train_set.to_pickle('train.csv')\n",
    "#test_set.to_pickle('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reads train pkl created above, and joins with the weather data on data and origin/destination.\n",
    "Saves combined data to data.csv\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge(df_air, name):\n",
    "\n",
    "    df_weath = pd.read_pickle('Weather.pkl')\n",
    "    df_weath = df_weath.rename(columns={\"NAME\":\"ORIGIN\", \"DATE\":\"FL_DATE\"})\n",
    "    df_weath['FL_DATE'] = pd.to_datetime(df_weath['FL_DATE']).dt.strftime('%Y-%m-%d')\n",
    "    df = df_air.merge(df_weath, on=['ORIGIN','FL_DATE'], how='outer')\n",
    "    df_weath = df_weath.rename(columns={\"ORIGIN\":\"DEST\", \"DATE\":\"FL_DATE\"})\n",
    "    df = df.merge(df_weath, on=['DEST','FL_DATE'], how='outer')\n",
    "    df = df.fillna(0)\n",
    "    df = df[df['ORIGIN'] != 0]\n",
    "    df = df[df['DEST'] != 0]\n",
    "    df.to_csv(name)\n",
    "    return df\n",
    "\n",
    "X = merge(train_set,'train.csv')\n",
    "X_test = merge(test_set, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create X vector, with only the data from df that is already real value or boolean encoded \n",
    "\"\"\"\n",
    "def clean(df):\n",
    "    remove = []\n",
    "    remove.append(df.columns[0])\n",
    "    for rem in df.columns[6:37]:\n",
    "        remove.append(rem)\n",
    "    remove.extend([\"WDF2_x\", \"WDF5_x\", \"WSF2_x\", \"WSF5_x\", \"WDF2_y\", \"WDF5_y\", \"WSF2_y\", \"WSF5_y\", \"STATION_y\",\n",
    "                   \"FL_DATE\", \"OP_UNIQUE_CARRIER\", \"ORIGIN\", \"DEST\", \"CRS_DEP_TIME\"])\n",
    "    return df.drop(remove, axis = 1)\n",
    "\n",
    "X = clean(X)\n",
    "X_test = clean(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates one-hot encodings for Airline, Origin, and Destination and concatenates the features to X\n",
    "\"\"\"\n",
    "def onehot(df, column):\n",
    "    title = df[column].unique()\n",
    "    header = [column + s for s in title]\n",
    "    onehot = []\n",
    "    for i in df[column]:\n",
    "        result = []\n",
    "        for j in title:\n",
    "            if i == j:\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "        onehot.append(result)\n",
    "    return pd.DataFrame(onehot, columns = header)\n",
    "\n",
    "X = pd.concat([X, onehot(train_set, \"OP_UNIQUE_CARRIER\"), onehot(train_set, \"ORIGIN\"), onehot(train_set, \"DEST\")],\n",
    "              axis = 1)\n",
    "X_test = pd.concat([X_test, onehot(test_set, \"OP_UNIQUE_CARRIER\"), onehot(test_set, \"ORIGIN\"),\n",
    "                    onehot(test_set,\"DEST\")], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(df, feature, max_order):\n",
    "    current_order = 2\n",
    "    transformed, names = [], []\n",
    "    while current_order <= max_order:\n",
    "        transformed.append(np.power(df[feature],current_order).values.flatten())\n",
    "        names.append(feature + \"^\" + str(current_order))\n",
    "        current_order += 1\n",
    "    return pd.DataFrame(transformed, index=names).T\n",
    "\n",
    "X = pd.concat([polynomial(X,'DEP_TIME',3), X],axis=1)\n",
    "X_test = pd.concat([polynomial(X_test,'DEP_TIME',3), X_test],axis=1)\n",
    "X['OFFSET'] = [1] * len(X.index)\n",
    "X_test['OFFSET'] = [1] *len(X_test.index)\n",
    "X.to_csv('X.csv')\n",
    "X_test.to_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build NAS delay vectors for clustering.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "train_set = pd.read_csv(\"train.csv\")\n",
    "test_set = pd.read_csv(\"test.csv\")\n",
    "vals = []\n",
    "for index, row in train_set.iterrows():\n",
    "    if(row[\"NAS_DELAY\"]>0):\n",
    "        vals.append(row[\"NAS_DELAY\"]) \n",
    "    else:\n",
    "        vals.append(0)\n",
    "Yclass_train = pd.DataFrame(vals)\n",
    "\n",
    "\n",
    "vals = []\n",
    "for index, row in test_set.iterrows():\n",
    "    if(row[\"NAS_DELAY\"]>0):\n",
    "        vals.append(row[\"NAS_DELAY\"])\n",
    "    else:\n",
    "        vals.append(0)\n",
    "Yclass_test = pd.DataFrame(vals)\n",
    "\n",
    "Yclass_train.to_csv(\"NAS_train.csv\")\n",
    "Yclass_test.to_csv(\"NAS_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology: First we must create a model to classify our data into two groups, one for weather delay and one for no weather delay. A later extension of this would be to do a multi-class classification: Severe Weather Delay, NAS Weather Delay, and No Weather Delay. Once we have classified our data, we must then use a regression model to predict the expected time of the dealay.\n",
    "\n",
    "Building Y Vector: The data we have gives us time of delay due to \"Severe Weather\" and time of delay due to \"National Air System\". Of these NAS delays, we know 55% of them are due to weather. To figure out, which 55% of these delays are the ones due to weather, we must run an unspervised clustering method on the weather features of the data. Once we have our clusters, we select the clusters with the highest fraction of points that have an NAS delay. These clusters are likely to have the NAS delay cause by weather. We then select clusters in descending order until we have 55% of the data points selected. These are the NAS delays that we accept as caused by weather.\n",
    "\n",
    "Overfitting/Underfitting: We have devised two solutions to ensuring that our model will generalize well. Firstly, we have broken the model into two parts: first a classification problem, and then a regression problem. By breaking the problem into two parts we help resolve the issue of zero-inflated data set. Secondly, we can throw out some examples of no weather delay in the classification model, to ensure that we do not overfit to the data.Lastly, we sampled our data randomly from each month of the year 2018, to ensure we have a sample with weather representative of all parts of the year. Because there is such a high volume of flights daily, we have a very large amount of data available to us, so expanding training and test set sizes is always feasible, providing another way to ensure the model generalizes well.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
